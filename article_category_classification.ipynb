{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "# keras tensorflow wrapper\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.applications import InceptionV3, Xception\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "# scikit-learn helper function\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_accuracy(true, pred):\n",
    "    return top_k_categorical_accuracy(true, pred, k=3)\n",
    "\n",
    "def path_join(dirname, img_paths):\n",
    "    return [os.path.join(dirname, img_path) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 데이터 전처리 및 generator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 673383 images belonging to 20 classes.\n",
      "Found 74813 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = '../training'\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             validation_split=0.1)\n",
    "\n",
    "batch_size = 32\n",
    "input_shape = (224,224)\n",
    "\n",
    "generator_train = datagen.flow_from_directory(directory=TRAIN_PATH,\n",
    "                                              target_size=input_shape,\n",
    "                                              shuffle=True,\n",
    "                                              subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "generator_validate = datagen.flow_from_directory(directory=TRAIN_PATH,\n",
    "                                                 target_size=input_shape,\n",
    "                                                 shuffle=False,\n",
    "                                                 subset=\"validation\"\n",
    "                                                 )\n",
    "steps_train = generator_train.n / batch_size\n",
    "steps_validate = generator_validate.n / batch_size\n",
    "\n",
    "cls_train = generator_train.classes\n",
    "cls_validate = generator_validate.classes\n",
    "\n",
    "num_classes = generator_train.num_classes\n",
    "\n",
    "class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                    classes=np.unique(cls_train),\n",
    "                                    y=cls_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의 및 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, name, class_weight, params):\n",
    "        assert name != '', \"Model name needs to be specified\"\n",
    "        self.name = name\n",
    "        self.trained = False\n",
    "        # feature extraction\n",
    "        \n",
    "    def construct_model(self):\n",
    "        if self.name == 'inceptionv3':\n",
    "            print('{:=^75}'.format('Downloading {}'.format(self.name)))\n",
    "            self.base_model = InceptionV3(**params['network_params'])\n",
    "            print('{:=^75}'.format('Download Complete'))\n",
    "            \n",
    "        elif self.name == 'xception':\n",
    "            print('{:=^75}'.format('Downloading {}'.format(self.name)))\n",
    "            self.base_model = Xception(**params['network_params'])\n",
    "            print('{:=^75}'.format('Download Complete'))\n",
    "            \n",
    "            \n",
    "        # 모델 구조  base model -> global average pooling -> dense\n",
    "        print('{:=^75}'.format('Adding layers'))\n",
    "        self.model = Sequential()\n",
    "        self.model.add(self.base_model)\n",
    "        self.model.add(GlobalAveragePooling2D())\n",
    "        self.model.add(Dense(params['num_classes'], activation='softmax'))\n",
    "        print('{:=^75}'.format('Added layers'))\n",
    "        \n",
    "        if params['mode'] == 'fe':\n",
    "            self.model.layers[0].trainable = False\n",
    "            \n",
    "        # finetuning\n",
    "        elif params['mode'] == 'ft':\n",
    "            self.model.layers[0].trainable = True \n",
    "        \n",
    "        # 지정 경로에 저장\n",
    "        if not os.path.exists('weight_path/'):\n",
    "            os.mkdir('weight_path/')\n",
    "        self.weight_save_path = os.path.join('weight_path/', self.name + \"_weights.h5\")\n",
    "        \n",
    "        print('{:=^75}'.format('Saving weights to {}'.format(self.weight_save_path)))\n",
    "        self.model.save_weights(self.weight_save_path)\n",
    "        print('{:=^75}'.format('Saved weights'))\n",
    "    \n",
    "    \n",
    "    # train with feature extraction\n",
    "    def train(self):\n",
    "        if self.trained == True:\n",
    "            self.model.load_weights(self.weight_save_path)\n",
    "            self.trained = False\n",
    "        \n",
    "        assert params['mode'] in ['fe', 'ft'], \"mode must be either 'fe' or 'ft'\"  \n",
    "            \n",
    "        # compile the model with designated parameters    \n",
    "        self.model.compile(optimizer=Adam(lr=params['lr']),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['categorical_accuracy', top_3_accuracy])\n",
    "        \n",
    "        if not os.path.exists(params['log_path']):\n",
    "            os.mkdir(params['log_path'])\n",
    "        \n",
    "        if not os.path.exists(params['cp_path']):\n",
    "            os.mkdir(params['cp_path'])\n",
    "        \n",
    "        # csv logger callback \n",
    "        log_path = os.path.join(params['log_path'], self.name + '_' + params['mode'] + '.log')\n",
    "        csvlog_callback = CSVLogger(log_path)\n",
    "        \n",
    "        # checkpoint callback \n",
    "        cp_path = os.path.join(params['cp_path'], self.name + '_' + params['mode'] + '-{epoch:04d}-{val_loss:.2f}.h5')\n",
    "        cp_callback = ModelCheckpoint(cp_path,\n",
    "                                      mode=\"max\",\n",
    "                                      save_best_only=True)\n",
    "        \n",
    "        print('{:=^75}'.format('training {} with {} mode'.format(self.name, params['mode'])))\n",
    "        # actual data fitting\n",
    "        self.model.fit_generator(generator=generator_train,\n",
    "                                  epochs=params['epoch'],\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_validate,\n",
    "                                  validation_steps=steps_validate,\n",
    "                                  callbacks=[cp_callback, csvlog_callback])\n",
    "        \n",
    "        # save model once done training    \n",
    "        if not os.path.exists(params['model_path']):\n",
    "            os.mkdir(params['model_path'])\n",
    "            \n",
    "        model_save_path = os.path.join(params['model_path'], model.name + '_' + params['mode'] + '.h5')\n",
    "        self.model.save(model_save_path)\n",
    "        self.trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_classes': num_classes,\n",
    "    'log_path': 'log/',\n",
    "    'cp_path': 'checkpoint/',\n",
    "    'model_path': 'model/',\n",
    "    'mode': 'fe',\n",
    "    'lr': 0.0001,\n",
    "    'epoch': 10,\n",
    "    'network_params': {\n",
    "    'include_top' : False, \n",
    "    'weights' : 'imagenet', \n",
    "    'input_shape' : input_shape + (3,)\n",
    "    }\n",
    "}\n",
    "\n",
    "inception = Model(name='inceptionv3', class_weight=class_weight, params=params)\n",
    "xception = Model(name='xception', class_weight=class_weight, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Downloading inceptionv3==========================\n",
      "=============================Download Complete=============================\n",
      "===============================Adding layers===============================\n",
      "===============================Added layers================================\n",
      "===========Saving weights to weight_path/inceptionv3_weights.h5============\n",
      "===============================Saved weights===============================\n"
     ]
    }
   ],
   "source": [
    "inception.construct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================Downloading xception============================\n",
      "=============================Download Complete=============================\n",
      "===============================Adding layers===============================\n",
      "===============================Added layers================================\n",
      "=============Saving weights to weight_path/xception_weights.h5=============\n",
      "===============================Saved weights===============================\n"
     ]
    }
   ],
   "source": [
    "xception.construct_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                40980     \n",
      "=================================================================\n",
      "Total params: 21,843,764\n",
      "Trainable params: 21,809,332\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inception.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 7, 7, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                40980     \n",
      "=================================================================\n",
      "Total params: 20,902,460\n",
      "Trainable params: 40,980\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xception.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================training with===============================\n",
      "Epoch 1/10\n",
      "21044/21044 [==============================] - 2784s 132ms/step - loss: 1.3867 - categorical_accuracy: 0.5516 - top_3_accuracy: 0.8290 - val_loss: 1.2923 - val_categorical_accuracy: 0.5826 - val_top_3_accuracy: 0.8481\n",
      "Epoch 2/10\n",
      "21044/21044 [==============================] - 2956s 140ms/step - loss: 1.3049 - categorical_accuracy: 0.5738 - top_3_accuracy: 0.8469 - val_loss: 1.2685 - val_categorical_accuracy: 0.5879 - val_top_3_accuracy: 0.8527\n",
      "Epoch 3/10\n",
      "21044/21044 [==============================] - 2795s 133ms/step - loss: 1.2890 - categorical_accuracy: 0.5786 - top_3_accuracy: 0.8499 - val_loss: 1.2556 - val_categorical_accuracy: 0.5905 - val_top_3_accuracy: 0.8573\n",
      "Epoch 4/10\n",
      "  669/21044 [..............................] - ETA: 39:59 - loss: 1.2836 - categorical_accuracy: 0.5742 - top_3_accuracy: 0.8540"
     ]
    }
   ],
   "source": [
    "inception.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================training xception with fe mode=======================\n",
      "Epoch 1/10\n",
      "20514/21044 [============================>.] - ETA: 1:13 - loss: 1.3684 - categorical_accuracy: 0.5694 - top_3_accuracy: 0.8393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5597/21044 [======>.......................] - ETA: 31:53 - loss: 1.2746 - categorical_accuracy: 0.5918 - top_3_accuracy: 0.8576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11904/21044 [===============>..............] - ETA: 18:41 - loss: 1.2719 - categorical_accuracy: 0.5921 - top_3_accuracy: 0.8583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17459/21044 [=======================>......] - ETA: 7:20 - loss: 1.2674 - categorical_accuracy: 0.5932 - top_3_accuracy: 0.8593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21044/21044 [==============================] - 2892s 137ms/step - loss: 1.2655 - categorical_accuracy: 0.5937 - top_3_accuracy: 0.8595 - val_loss: 1.2304 - val_categorical_accuracy: 0.6043 - val_top_3_accuracy: 0.8653\n",
      "Epoch 3/10\n",
      " 2939/21044 [===>..........................] - ETA: 36:06 - loss: 1.2480 - categorical_accuracy: 0.5975 - top_3_accuracy: 0.8621"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8915/21044 [===========>..................] - ETA: 24:12 - loss: 1.2464 - categorical_accuracy: 0.5984 - top_3_accuracy: 0.8621"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15265/21044 [====================>.........] - ETA: 11:36 - loss: 1.2458 - categorical_accuracy: 0.5982 - top_3_accuracy: 0.8625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xception.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\n",
    "    'mode': 'ft',\n",
    "    'lr': 0.0001\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21043/21044 [============================>.] - ETA: 0s - loss: 1.3891 - categorical_accuracy: 0.5528 - top_3_accuracy: 0.8306"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-87225b4ad4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                           callbacks=[cp_callback, csvlog_callback])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlog_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'log/xception_fe.log'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    214\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m       \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "# compile 전 feature extraction에서는 dense를 제외한 모든 layer의 weight를 고정\n",
    "incep_model.layers[0].trainable = False\n",
    "xcep_model.layers[0].trainable = False\n",
    "\n",
    "# model 훈련전 compile을 실행\n",
    "incep_model.compile(optimizer=Adam(lr=0.001), \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['categorical_accuracy', top_3_accuracy])\n",
    "\n",
    "xcep_model.compile(optimizer=Adam(lr=0.001),\n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['categorical_accuracy', top_3_accuracy])\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                   classes=np.unique(cls_train),\n",
    "                                   y=cls_train)\n",
    "\n",
    "log_path = 'log/inception_fe.log'\n",
    "checkpoint_path = \"checkpoint/model-{epoch:04d}-{val_acc:.2f}.h5\"\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                              mode=\"max\",\n",
    "                              save_best_only=True)\n",
    "\n",
    "csvlog_callback = CSVLogger(log_path)\n",
    "incep_model.fit_generator(generator=generator_train,\n",
    "                          epochs=epochs,\n",
    "                          class_weight=class_weight,\n",
    "                          validation_data=generator_validate,\n",
    "                          validation_steps=steps_validate,\n",
    "                          callbacks=[cp_callback, csvlog_callback])\n",
    "\n",
    "log_path = 'log/xception_fe.log'\n",
    "csvlog_callback = CSVLogger(log_path)\n",
    "checkpoint_path = \"checkpoint/model-{epoch:04d}-{val_acc:.2f}.h5\"\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                              mode=\"max\",\n",
    "                              save_best_only=True)\n",
    "xcep_model.fit_generator(generator=generator_train,\n",
    "                          epochs=epochs,\n",
    "                          class_weight=class_weight,\n",
    "                          validation_data=generator_validate,\n",
    "                          validation_steps=steps_validate)\n",
    "\n",
    "# 훈련이 끝난 모델을 지정한 경로에 저장\n",
    "incep_model.save(model_save_path + 'inceptionv3.h5')\n",
    "xcep_model.save(model_save_path + 'xception.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 끝난 모델의 weight를 초기 weight로 다시 불러옴\n",
    "incep_model.load_weights('inceptionv3_weights.h5')\n",
    "xcep_model.load_weights('xception_weights.h5')\n",
    "\n",
    "# fine tuning에서는 모든 layer의 weight를 update\n",
    "incep_model.layers[0].trainable=True\n",
    "xcep_model.layers[0].trainable=True\n",
    "\n",
    "# fine tuning에서는 learning rate를 낮춰서 훈련 & recompile\n",
    "incep_model.compile(optimizer=Adam(lr=0.0001), \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['categorical_accuracy', top_3_accuracy])\n",
    "\n",
    "xcep_model.compile(optimizer=Adam(lr=0.0001),\n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['categorical_accuracy', top_3_accuracy])\n",
    "\n",
    "\n",
    "log_path = 'log/inception_ft.log'\n",
    "incep_model.fit_generator(generator=generator_train,\n",
    "                          epochs=epochs,\n",
    "                          class_weight=class_weight,\n",
    "                          validation_data=generator_validate,\n",
    "                          validation_steps=steps_validate,\n",
    "                          callbacks=[cp_callback, csvlog_callback])\n",
    "\n",
    "log_path = 'log/xception_ft.log'\n",
    "xcep_model.fit_generator(generator=generator_train,\n",
    "                          epochs=epochs,\n",
    "                          class_weight=class_weight,\n",
    "                          validation_data=generator_validate,\n",
    "                          validation_steps=steps_validate)\n",
    "\n",
    "# 훈련이 끝난 모델을 지정한 경로에 저장\n",
    "incep_model.save(model_save_path + 'inceptionv3_finetune.h5')\n",
    "xcep_model.save(model_save_path + 'xception_finetune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
